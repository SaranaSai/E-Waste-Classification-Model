{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaranaSai/E-Waste-Classification-Model/blob/main/E_Waste_Final_Lite_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio scikit-learn\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetV2B1\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input, decode_predictions\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import zipfile, io, tempfile\n",
        "from google.colab import files # Import files for Colab upload\n",
        "import shutil # Import shutil for moving files\n",
        "from sklearn.model_selection import train_test_split # Import for splitting\n",
        "\n",
        "# --- Add dataset setup here ---\n",
        "def setup_dataset_for_second_cell():\n",
        "    print(\"üìÅ Please upload a ZIP file containing e-waste images.\")\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            raise ValueError(\"No file uploaded!\")\n",
        "\n",
        "        zip_path = list(uploaded.keys())[0]\n",
        "        temp_unzip_path = \"/content/uploaded_dataset_temp\" # Unzip to a temporary path first\n",
        "        target_base_path = \"/content/modified-dataset\" # Target path for train/val/test structure\n",
        "\n",
        "        # Clean up previous runs\n",
        "        if os.path.exists(temp_unzip_path):\n",
        "            shutil.rmtree(temp_unzip_path)\n",
        "        if os.path.exists(target_base_path):\n",
        "            shutil.rmtree(target_base_path)\n",
        "\n",
        "        os.makedirs(temp_unzip_path, exist_ok=True)\n",
        "\n",
        "        print(f\"Unzipping {zip_path}...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(temp_unzip_path)\n",
        "        print(\"Unzipping complete.\")\n",
        "\n",
        "        # Function to find directories containing image files\n",
        "        def find_image_directories(base_dir):\n",
        "            image_dirs = []\n",
        "            for root, dirs, files in os.walk(base_dir):\n",
        "                if any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in files):\n",
        "                    image_dirs.append(root)\n",
        "            return image_dirs\n",
        "\n",
        "        potential_class_dirs = find_image_directories(temp_unzip_path)\n",
        "\n",
        "        if not potential_class_dirs:\n",
        "             raise ValueError(\"No directories containing image files found in the zip.\")\n",
        "\n",
        "        # Group images by detected class directory\n",
        "        class_data = {}\n",
        "        for class_dir in potential_class_dirs:\n",
        "            # Use the last part of the path as the class name\n",
        "            class_name = os.path.basename(class_dir) or \"unknown_class\" # Handle root directory case\n",
        "            images = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.lower().endswith(('jpg', 'jpeg', 'png'))]\n",
        "            if images:\n",
        "                 class_data[class_name] = images\n",
        "\n",
        "        if len(class_data) < 2:\n",
        "            raise ValueError(\"At least 2 directories containing images are required to define classes.\")\n",
        "\n",
        "        total_images = sum(len(imgs) for imgs in class_data.values())\n",
        "        # Increased minimum images for better training\n",
        "        MIN_TOTAL_IMAGES = 30 # Recommended minimum, adjust based on complexity\n",
        "        if total_images < MIN_TOTAL_IMAGES:\n",
        "            raise ValueError(f\"Dataset must contain at least {MIN_TOTAL_IMAGES} images (found {total_images}). Training may not be effective with fewer images.\")\n",
        "\n",
        "        print(f\"Found {len(class_data)} classes: {list(class_data.keys())}\")\n",
        "        print(f\"Total images found: {total_images}\")\n",
        "\n",
        "        # Now, prepare train/val/test splits into the target directories\n",
        "        train_path = os.path.join(target_base_path, \"train\")\n",
        "        val_path = os.path.join(target_base_path, \"val\")\n",
        "        test_path = os.path.join(target_base_path, \"test\")\n",
        "\n",
        "        os.makedirs(train_path, exist_ok=True)\n",
        "        os.makedirs(val_path, exist_ok=True)\n",
        "        os.makedirs(test_path, exist_ok=True)\n",
        "\n",
        "        split_ratios = {'train': 0.7, 'val': 0.15, 'test': 0.15} # Adjust ratios as needed\n",
        "\n",
        "        print(\"Splitting data and creating train/val/test directories...\")\n",
        "        for class_name, image_paths in class_data.items():\n",
        "            # Create class directories in target folders\n",
        "            os.makedirs(os.path.join(train_path, class_name), exist_ok=True)\n",
        "            os.makedirs(os.path.join(val_path, class_name), exist_ok=True)\n",
        "            os.makedirs(os.path.join(test_path, class_name), exist_ok=True)\n",
        "\n",
        "            # Split image paths\n",
        "            train_imgs, temp_imgs = train_test_split(image_paths, test_size=split_ratios['val'] + split_ratios['test'], random_state=42, shuffle=True)\n",
        "            val_imgs, test_imgs = train_test_split(temp_imgs, test_size=split_ratios['test'] / (split_ratios['val'] + split_ratios['test']), random_state=42, shuffle=True) # Correct ratio calculation\n",
        "\n",
        "            # Copy files to target directories\n",
        "            for img_path in train_imgs:\n",
        "                shutil.copy(img_path, os.path.join(train_path, class_name, os.path.basename(img_path)))\n",
        "            for img_path in val_imgs:\n",
        "                shutil.copy(img_path, os.path.join(val_path, class_name, os.path.basename(img_path)))\n",
        "            for img_path in test_imgs:\n",
        "                shutil.copy(img_path, os.path.join(test_path, class_name, os.path.basename(img_path)))\n",
        "        print(\"Data splitting and copying complete.\")\n",
        "\n",
        "        # Clean up temporary unzip path\n",
        "        shutil.rmtree(temp_unzip_path)\n",
        "\n",
        "        print(f\"‚úÖ Dataset prepared in: {target_base_path}\")\n",
        "        return train_path, val_path, test_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error setting up dataset for second model: {e}\")\n",
        "        # Clean up if an error occurred\n",
        "        if os.path.exists(temp_unzip_path):\n",
        "            shutil.rmtree(temp_unzip_path)\n",
        "        if os.path.exists(target_base_path):\n",
        "             shutil.rmtree(target_base_path)\n",
        "        raise # Re-raise the exception so the following code doesn't run\n",
        "\n",
        "\n",
        "# Run dataset setup before loading\n",
        "train_path, val_path, test_path = None, None, None # Initialize paths to None\n",
        "try:\n",
        "    train_path, val_path, test_path = setup_dataset_for_second_cell()\n",
        "except Exception as e:\n",
        "    print(\"Skipping model training and Gradio interface due to dataset setup error.\")\n",
        "\n",
        "# STEP 2: Load Dataset with Augmentation\n",
        "IMAGE_SIZE = (240, 240)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "augment_ops = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1)\n",
        "])\n",
        "\n",
        "datasets_loaded_successfully = False\n",
        "if train_path and val_path and test_path and os.path.exists(train_path) and os.path.exists(val_path) and os.path.exists(test_path):\n",
        "    try:\n",
        "        print(\"Loading datasets...\")\n",
        "        # image_dataset_from_directory handles the basic image loading and resizing\n",
        "        datatrain = image_dataset_from_directory(train_path, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, label_mode='categorical')\n",
        "        dataval = image_dataset_from_directory(val_path, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, label_mode='categorical')\n",
        "        datatest = image_dataset_from_directory(test_path, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, label_mode='categorical')\n",
        "        print(\"Datasets loaded.\")\n",
        "\n",
        "        # Check if datasets are empty\n",
        "        if len(datatrain) > 0 and len(dataval) > 0 and len(datatest) > 0:\n",
        "             class_names = datatrain.class_names\n",
        "             NUM_CLASSES = len(class_names)\n",
        "\n",
        "             AUTOTUNE = tf.data.AUTOTUNE\n",
        "             # Apply preprocessing first\n",
        "             datatrain = datatrain.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n",
        "             # Then apply augmentation to the preprocessed data\n",
        "             datatrain = datatrain.map(lambda x, y: (augment_ops(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
        "             # Cache, shuffle, prefetch\n",
        "             datatrain = datatrain.cache().shuffle(buffer_size=min(1000, len(datatrain)*BATCH_SIZE)).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "             # Apply preprocessing to validation and test datasets (NO augmentation)\n",
        "             dataval = dataval.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n",
        "             dataval = dataval.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "             datatest = datatest.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n",
        "             datatest = datatest.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "             datasets_loaded_successfully = True\n",
        "        else:\n",
        "            print(\"Error: One or more datasets are empty after splitting.\")\n",
        "            # Clean up the target_base_path if datasets are empty\n",
        "            target_base_path = \"/content/modified-dataset\" # Define again if needed\n",
        "            if os.path.exists(target_base_path):\n",
        "                shutil.rmtree(target_base_path)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading and preprocessing datasets: {e}\")\n",
        "        # Clean up the target_base_path if dataset loading fails\n",
        "        target_base_path = \"/content/modified-dataset\" # Define again if needed\n",
        "        if os.path.exists(target_base_path):\n",
        "            shutil.rmtree(target_base_path)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Dataset paths not found or not set up correctly. Skipping dataset loading.\")\n",
        "\n",
        "\n",
        "# Ensure datasets were loaded successfully before proceeding with model training/evaluation/Gradio\n",
        "if datasets_loaded_successfully:\n",
        "\n",
        "    # STEP 3: Build Model\n",
        "    print(\"Building model...\")\n",
        "    base_model = EfficientNetV2B1(include_top=False, input_shape=(*IMAGE_SIZE, 3), weights='imagenet')\n",
        "    # Keep base model frozen initially for transfer learning\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = Sequential([\n",
        "        layers.Input(shape=(*IMAGE_SIZE, 3)),\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(), # Add BatchNormalization\n",
        "        layers.Dropout(0.4), # Slightly increase dropout\n",
        "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Use AdamW optimizer for potentially better performance\n",
        "    model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-3), # Higher learning rate for the head\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    print(\"Model built.\")\n",
        "\n",
        "    # STEP 4: Callbacks\n",
        "    # Save model in the new Keras format (.keras)\n",
        "    model_ckpt = callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "    # Early stopping based on validation loss\n",
        "    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True) # Increased patience slightly\n",
        "    # Reduce learning rate on plateau\n",
        "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-6) # Increased patience, decreased factor\n",
        "\n",
        "\n",
        "    callbacks_list = [early_stop, reduce_lr, model_ckpt]\n",
        "\n",
        "\n",
        "    # STEP 5: Training (Head only)\n",
        "    print(\"Starting training (head only)...\")\n",
        "    # Calculate epochs based on training dataset size, adjust min/max\n",
        "    initial_epochs = min(20, max(10, len(datatrain) * 2)) # Train head for more epochs initially\n",
        "    try:\n",
        "        history = model.fit(datatrain, validation_data=dataval, epochs=initial_epochs, callbacks=callbacks_list)\n",
        "        print(\"Initial training complete.\")\n",
        "\n",
        "        model = tf.keras.models.load_model(\"best_model.keras\")\n",
        "        print(\"Loaded best model for fine-tuning.\")\n",
        "\n",
        "\n",
        "        # STEP 6: Fine-Tuning (Unfreeze some layers)\n",
        "        print(\"Starting fine-tuning...\")\n",
        "        # Unfreeze the base model\n",
        "        base_model.trainable = True\n",
        "\n",
        "        fine_tune_layers = 30 # Adjust based on model size and dataset size. More layers for larger datasets.\n",
        "        for layer in base_model.layers[:-fine_tune_layers]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Recompile the model with a lower learning rate for fine-tuning\n",
        "        model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-5), # Very low learning rate for fine-tuning\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "\n",
        "        total_epochs = initial_epochs + fine_tune_epochs # Keep track of total epochs if needed for plotting\n",
        "\n",
        "        # Use the same callbacks for fine-tuning, EarlyStopping will monitor val_loss\n",
        "        history_ft = model.fit(datatrain, validation_data=dataval, epochs=total_epochs,\n",
        "                               initial_epoch=history.epoch[-1] + 1, # Start from where initial training ended\n",
        "                               callbacks=callbacks_list)\n",
        "        print(\"Fine-tuning complete.\")\n",
        "\n",
        "        # Concatenate histories for plotting\n",
        "        def combine_histories(h1, h2):\n",
        "            history = {}\n",
        "            for key in h1.history.keys():\n",
        "                history[key] = h1.history[key] + h2.history[key]\n",
        "            return history\n",
        "\n",
        "        full_history = combine_histories(history, history_ft)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during training or fine-tuning: {e}\")\n",
        "        # Set history to None if training failed completely\n",
        "        full_history = None\n",
        "\n",
        "\n",
        "    # STEP 7: Plot Training History (if training was successful)\n",
        "    if full_history:\n",
        "        print(\"Plotting training history...\")\n",
        "        plt.figure(figsize=(14, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(full_history['accuracy'], label='Train Acc')\n",
        "        if 'val_accuracy' in full_history:\n",
        "            plt.plot(full_history['val_accuracy'], label='Val Acc')\n",
        "        plt.title(\"Accuracy\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(full_history['loss'], label='Train Loss')\n",
        "        if 'val_loss' in full_history:\n",
        "            plt.plot(full_history['val_loss'], label='Val Loss')\n",
        "        plt.title(\"Loss\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Skipping history plotting as training failed.\")\n",
        "\n",
        "\n",
        "    # STEP 8: Evaluation with Test-Time Augmentation (TTA) (if test dataset exists)\n",
        "    if 'datatest' in locals() and datatest is not None:\n",
        "        print(\"Evaluating model with TTA...\")\n",
        "\n",
        "        def predict_tta(model, dataset, n_aug=3):\n",
        "            all_preds, all_labels = [], []\n",
        "            if dataset is None:\n",
        "                 print(\"Test dataset is not available for TTA.\")\n",
        "                 return [], []\n",
        "\n",
        "            total_batches = len(list(dataset))\n",
        "            if total_batches == 0:\n",
        "                print(\"Test dataset is empty for TTA.\")\n",
        "                return [], []\n",
        "\n",
        "            print(f\"  Running TTA with {n_aug} augmentations per image...\")\n",
        "            for i, (images, labels) in enumerate(dataset):\n",
        "                # The images from the dataset pipeline are already preprocessed\n",
        "                tta_preds = []\n",
        "                try:\n",
        "                    for _ in range(n_aug):\n",
        "\n",
        "                        aug_imgs = augment_ops(images, training=True) # Apply augmentation\n",
        "                        preds = model.predict(aug_imgs, verbose=0)\n",
        "                        tta_preds.append(preds)\n",
        "                    avg_preds = np.mean(tta_preds, axis=0)\n",
        "                    all_preds.extend(np.argmax(avg_preds, axis=1))\n",
        "                    all_labels.extend(np.argmax(labels.numpy(), axis=1))\n",
        "                except Exception as e:\n",
        "                     print(f\"  Error during TTA for batch {i}: {e}\")\n",
        "                     # Optionally skip this batch or log the error\n",
        "                     continue # Skip to the next batch on error\n",
        "\n",
        "            print(\"Evaluation complete.\")\n",
        "            return all_preds, all_labels\n",
        "\n",
        "        # Call predict_tta only if datatest was successfully loaded\n",
        "        if 'datatest' in locals() and datatest is not None and len(list(datatest)) > 0:\n",
        "             preds, labels = predict_tta(model, datatest)\n",
        "\n",
        "             if preds and labels:\n",
        "                 print(classification_report(labels, preds, target_names=class_names))\n",
        "\n",
        "                 # STEP 9: Confusion Matrix\n",
        "                 cm = confusion_matrix(labels, preds)\n",
        "                 plt.figure(figsize=(10, 8))\n",
        "                 sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
        "                 plt.xlabel('Predicted')\n",
        "                 plt.ylabel('True')\n",
        "                 plt.title('Confusion Matrix')\n",
        "                 plt.show()\n",
        "             else:\n",
        "                 print(\"Skipping classification report and confusion matrix due to empty prediction results.\")\n",
        "        else:\n",
        "             print(\"Skipping evaluation as test dataset is not available or is empty.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Skipping evaluation as test dataset is not available.\")\n",
        "\n",
        "\n",
        "   # STEP 10: Gradio App (if model and class names are available)\n",
        "if 'model' in locals() and 'class_names' in locals() and model is not None and class_names is not None:\n",
        "    print(\"Launching Gradio interface...\")\n",
        "\n",
        "    def process_image_for_gradio(img_pil, img_width, img_height, num_classes, model, class_names):\n",
        "        try:\n",
        "            img = img_pil.convert(\"RGB\").resize((img_width, img_height))\n",
        "            img_array = tf.keras.utils.img_to_array(img)\n",
        "            img_array = tf.expand_dims(img_array, 0)\n",
        "            img_array = preprocess_input(img_array)\n",
        "            predictions = model.predict(img_array, verbose=0)[0]\n",
        "\n",
        "            if num_classes == 2:\n",
        "                score = float(predictions[0])\n",
        "                return {\n",
        "                    class_names[1]: round(score, 4),\n",
        "                    class_names[0]: round(1 - score, 4)\n",
        "                }\n",
        "            else:\n",
        "                probs = tf.nn.softmax(predictions).numpy()\n",
        "                return {class_names[i]: round(float(probs[i]), 4) for i in range(num_classes)}\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error processing image: {e}\"\n",
        "\n",
        "    def classify_upload(file):\n",
        "        results = []\n",
        "\n",
        "        if file is None:\n",
        "            return pd.DataFrame(columns=[\"Filename\", \"Prediction\"]), None\n",
        "\n",
        "        if zipfile.is_zipfile(file.name):\n",
        "            with zipfile.ZipFile(file.name) as archive:\n",
        "                for entry in archive.namelist():\n",
        "                    if entry.endswith('/') or not entry.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        continue\n",
        "                    try:\n",
        "                        with archive.open(entry) as f:\n",
        "                            img_data = f.read()\n",
        "                            img_pil = Image.open(io.BytesIO(img_data))\n",
        "                            prediction_result = process_image_for_gradio(img_pil, IMAGE_SIZE[1], IMAGE_SIZE[0], NUM_CLASSES, model, class_names)\n",
        "\n",
        "                            if isinstance(prediction_result, str) and prediction_result.startswith(\"Error\"):\n",
        "                                results.append((entry, prediction_result))\n",
        "                            else:\n",
        "                                top_class = max(prediction_result, key=prediction_result.get)\n",
        "                                results.append((entry, top_class))\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error handling zip entry {entry}: {e}\")\n",
        "                        results.append((entry, f\"Error reading file: {e}\"))\n",
        "        else:\n",
        "            try:\n",
        "                img_pil = Image.open(file.name)\n",
        "                prediction_result = process_image_for_gradio(img_pil, IMAGE_SIZE[1], IMAGE_SIZE[0], NUM_CLASSES, model, class_names)\n",
        "\n",
        "                if isinstance(prediction_result, str) and prediction_result.startswith(\"Error\"):\n",
        "                    results.append((os.path.basename(file.name), prediction_result))\n",
        "                else:\n",
        "                    top_class = max(prediction_result, key=prediction_result.get)\n",
        "                    results.append((os.path.basename(file.name), top_class))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error handling single image file {os.path.basename(file.name)}: {e}\")\n",
        "                results.append((os.path.basename(file.name), f\"Error opening file: {e}\"))\n",
        "\n",
        "        df = pd.DataFrame(results, columns=[\"Filename\", \"Prediction\"])\n",
        "        if not df.empty:\n",
        "            csv_path = tempfile.mktemp(suffix=\".csv\")\n",
        "            df.to_csv(csv_path, index=False)\n",
        "            return df, csv_path\n",
        "        else:\n",
        "            print(\"No valid images processed for classification.\")\n",
        "            return df, None\n",
        "\n",
        "    gr.Interface(\n",
        "        fn=classify_upload,\n",
        "        inputs=gr.File(label=\"Upload image or ZIP\", file_types=[\".zip\", \".jpg\", \".jpeg\", \".png\"]),\n",
        "        outputs=[gr.Dataframe(), gr.File(label=\"Download Results\")],\n",
        "        title=\"E-Waste Image Classifier\",\n",
        "    ).launch(share=True, debug=True)\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Gradio interface as model or class names are not available.\")\n"
      ],
      "metadata": {
        "id": "C-BjAIoH5gjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+YlmFgKMcNDyZSW8cE1Bm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}