{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio scikit-learn\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetV2B1\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input, decode_predictions\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import zipfile, io, tempfile\n",
        "from google.colab import files # Import files for Colab upload\n",
        "import shutil # Import shutil for moving files\n",
        "from sklearn.model_selection import train_test_split # Import for splitting\n",
        "\n",
        "# --- Add dataset setup here ---\n",
        "def setup_dataset_for_second_cell():\n",
        "    print(\"üìÅ Please upload a ZIP file containing e-waste images.\")\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            raise ValueError(\"No file uploaded!\")\n",
        "\n",
        "        zip_path = list(uploaded.keys())[0]\n",
        "        temp_unzip_path = \"/content/uploaded_dataset_temp\" # Unzip to a temporary path first\n",
        "        target_base_path = \"/content/modified-dataset\" # Target path for train/val/test structure\n",
        "\n",
        "        # Clean up previous runs\n",
        "        if os.path.exists(temp_unzip_path):\n",
        "            shutil.rmtree(temp_unzip_path)\n",
        "        if os.path.exists(target_base_path):\n",
        "            shutil.rmtree(target_base_path)\n",
        "\n",
        "        os.makedirs(temp_unzip_path, exist_ok=True)\n",
        "\n",
        "        print(f\"Unzipping {zip_path}...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(temp_unzip_path)\n",
        "        print(\"Unzipping complete.\")\n",
        "\n",
        "        # Function to find directories containing image files\n",
        "        def find_image_directories(base_dir):\n",
        "            image_dirs = []\n",
        "            for root, dirs, files in os.walk(base_dir):\n",
        "                if any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in files):\n",
        "                    image_dirs.append(root)\n",
        "            return image_dirs\n",
        "\n",
        "        potential_class_dirs = find_image_directories(temp_unzip_path)\n",
        "\n",
        "        if not potential_class_dirs:\n",
        "             raise ValueError(\"No directories containing image files found in the zip.\")\n",
        "\n",
        "        # Group images by detected class directory\n",
        "        class_data = {}\n",
        "        for class_dir in potential_class_dirs:\n",
        "            # Use the last part of the path as the class name\n",
        "            class_name = os.path.basename(class_dir) or \"unknown_class\" # Handle root directory case\n",
        "            images = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.lower().endswith(('jpg', 'jpeg', 'png'))]\n",
        "            if images:\n",
        "                 class_data[class_name] = images\n",
        "\n",
        "        if len(class_data) < 2:\n",
        "            raise ValueError(\"At least 2 directories containing images are required to define classes.\")\n",
        "\n",
        "        total_images = sum(len(imgs) for imgs in class_data.values())\n",
        "        # Increased minimum images for better training\n",
        "        MIN_TOTAL_IMAGES = 30 # Recommended minimum, adjust based on complexity\n",
        "        if total_images < MIN_TOTAL_IMAGES:\n",
        "            raise ValueError(f\"Dataset must contain at least {MIN_TOTAL_IMAGES} images (found {total_images}). Training may not be effective with fewer images.\")\n",
        "\n",
        "        print(f\"Found {len(class_data)} classes: {list(class_data.keys())}\")\n",
        "        print(f\"Total images found: {total_images}\")\n",
        "\n",
        "        # Now, prepare train/val/test splits into the target directories\n",
        "        train_path = os.path.join(target_base_path, \"train\")\n",
        "        val_path = os.path.join(target_base_path, \"val\")\n",
        "        test_path = os.path.join(target_base_path, \"test\")\n",
        "\n",
        "        os.makedirs(train_path, exist_ok=True)\n",
        "        os.makedirs(val_path, exist_ok=True)\n",
        "        os.makedirs(test_path, exist_ok=True)\n",
        "\n",
        "        split_ratios = {'train': 0.7, 'val': 0.15, 'test': 0.15} # Adjust ratios as needed\n",
        "\n",
        "        print(\"Splitting data and creating train/val/test directories...\")\n",
        "        for class_name, image_paths in class_data.items():\n",
        "            # Create class directories in target folders\n",
        "            os.makedirs(os.path.join(train_path, class_name), exist_ok=True)\n",
        "            os.makedirs(os.path.join(val_path, class_name), exist_ok=True)\n",
        "            os.makedirs(os.path.join(test_path, class_name), exist_ok=True)\n",
        "\n",
        "            # Split image paths\n",
        "            train_imgs, temp_imgs = train_test_split(image_paths, test_size=split_ratios['val'] + split_ratios['test'], random_state=42, shuffle=True)\n",
        "            val_imgs, test_imgs = train_test_split(temp_imgs, test_size=split_ratios['test'] / (split_ratios['val'] + split_ratios['test']), random_state=42, shuffle=True) # Correct ratio calculation\n",
        "\n",
        "            # Copy files to target directories\n",
        "            for img_path in train_imgs:\n",
        "                shutil.copy(img_path, os.path.join(train_path, class_name, os.path.basename(img_path)))\n",
        "            for img_path in val_imgs:\n",
        "                shutil.copy(img_path, os.path.join(val_path, class_name, os.path.basename(img_path)))\n",
        "            for img_path in test_imgs:\n",
        "                shutil.copy(img_path, os.path.join(test_path, class_name, os.path.basename(img_path)))\n",
        "        print(\"Data splitting and copying complete.\")\n",
        "\n",
        "        # Clean up temporary unzip path\n",
        "        shutil.rmtree(temp_unzip_path)\n",
        "\n",
        "        print(f\"‚úÖ Dataset prepared in: {target_base_path}\")\n",
        "        return train_path, val_path, test_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error setting up dataset for second model: {e}\")\n",
        "        # Clean up if an error occurred\n",
        "        if os.path.exists(temp_unzip_path):\n",
        "            shutil.rmtree(temp_unzip_path)\n",
        "        if os.path.exists(target_base_path):\n",
        "             shutil.rmtree(target_base_path)\n",
        "        raise # Re-raise the exception so the following code doesn't run\n",
        "\n",
        "\n",
        "# Run dataset setup before loading\n",
        "train_path, val_path, test_path = None, None, None # Initialize paths to None\n",
        "try:\n",
        "    train_path, val_path, test_path = setup_dataset_for_second_cell()\n",
        "except Exception as e:\n",
        "    print(\"Skipping model training and Gradio interface due to dataset setup error.\")\n",
        "\n",
        "# STEP 2: Load Dataset with Augmentation\n",
        "IMAGE_SIZE = (240, 240)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "augment_ops = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1)\n",
        "])\n",
        "\n",
        "datasets_loaded_successfully = False\n",
        "if train_path and val_path and test_path and os.path.exists(train_path) and os.path.exists(val_path) and os.path.exists(test_path):\n",
        "    try:\n",
        "        print(\"Loading datasets...\")\n",
        "        # image_dataset_from_directory handles the basic image loading and resizing\n",
        "        datatrain = image_dataset_from_directory(train_path, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, label_mode='categorical')\n",
        "        dataval = image_dataset_from_directory(val_path, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, label_mode='categorical')\n",
        "        datatest = image_dataset_from_directory(test_path, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, label_mode='categorical')\n",
        "        print(\"Datasets loaded.\")\n",
        "\n",
        "        # Check if datasets are empty\n",
        "        if len(datatrain) > 0 and len(dataval) > 0 and len(datatest) > 0:\n",
        "             class_names = datatrain.class_names\n",
        "             NUM_CLASSES = len(class_names)\n",
        "\n",
        "             AUTOTUNE = tf.data.AUTOTUNE\n",
        "             # Apply preprocessing first\n",
        "             datatrain = datatrain.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n",
        "             # Then apply augmentation to the preprocessed data\n",
        "             datatrain = datatrain.map(lambda x, y: (augment_ops(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
        "             # Cache, shuffle, prefetch\n",
        "             datatrain = datatrain.cache().shuffle(buffer_size=min(1000, len(datatrain)*BATCH_SIZE)).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "             # Apply preprocessing to validation and test datasets (NO augmentation)\n",
        "             dataval = dataval.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n",
        "             dataval = dataval.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "             datatest = datatest.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n",
        "             datatest = datatest.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "             datasets_loaded_successfully = True\n",
        "        else:\n",
        "            print(\"Error: One or more datasets are empty after splitting.\")\n",
        "            # Clean up the target_base_path if datasets are empty\n",
        "            target_base_path = \"/content/modified-dataset\" # Define again if needed\n",
        "            if os.path.exists(target_base_path):\n",
        "                shutil.rmtree(target_base_path)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading and preprocessing datasets: {e}\")\n",
        "        # Clean up the target_base_path if dataset loading fails\n",
        "        target_base_path = \"/content/modified-dataset\" # Define again if needed\n",
        "        if os.path.exists(target_base_path):\n",
        "            shutil.rmtree(target_base_path)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Dataset paths not found or not set up correctly. Skipping dataset loading.\")\n",
        "\n",
        "\n",
        "# Ensure datasets were loaded successfully before proceeding with model training/evaluation/Gradio\n",
        "if datasets_loaded_successfully:\n",
        "\n",
        "    # STEP 3: Build Model\n",
        "    print(\"Building model...\")\n",
        "    base_model = EfficientNetV2B1(include_top=False, input_shape=(*IMAGE_SIZE, 3), weights='imagenet')\n",
        "    # Keep base model frozen initially for transfer learning\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = Sequential([\n",
        "        layers.Input(shape=(*IMAGE_SIZE, 3)),\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(), # Add BatchNormalization\n",
        "        layers.Dropout(0.4), # Slightly increase dropout\n",
        "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Use AdamW optimizer for potentially better performance\n",
        "    model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-3), # Higher learning rate for the head\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    print(\"Model built.\")\n",
        "\n",
        "    # STEP 4: Callbacks\n",
        "    # Save model in the new Keras format (.keras)\n",
        "    model_ckpt = callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "    # Early stopping based on validation loss\n",
        "    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True) # Increased patience slightly\n",
        "    # Reduce learning rate on plateau\n",
        "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-6) # Increased patience, decreased factor\n",
        "\n",
        "\n",
        "    callbacks_list = [early_stop, reduce_lr, model_ckpt]\n",
        "\n"
      ],
      "metadata": {
        "id": "C-BjAIoH5gjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}